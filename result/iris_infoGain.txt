Info Gain (entropy)  Example


=== Attribute Selection on all input data ===

Search Method:
	Attribute ranking.

Attribute Evaluator (supervised, Class (nominal): 5 Species):
	Information Gain Ranking Filter

Ranked attributes:
 1.418  3 PetalLengthCm
 1.378  4 PetalWidthCm
 0.698  1 SepalLengthCm
 0.376  2 SepalWidthCm

Selected attributes: 3,4,1,2 : 4

2,3,0,1,4
5
No classifier built
J48 pruned tree
------------------

PetalWidthCm <= 0.6: Iris-setosa (50.0)
PetalWidthCm > 0.6
|   PetalWidthCm <= 1.7
|   |   PetalLengthCm <= 4.9: Iris-versicolor (48.0/1.0)
|   |   PetalLengthCm > 4.9
|   |   |   PetalWidthCm <= 1.5: Iris-virginica (3.0)
|   |   |   PetalWidthCm > 1.5: Iris-versicolor (3.0/1.0)
|   PetalWidthCm > 1.7: Iris-virginica (46.0/1.0)

Number of Leaves  : 	5

Size of the tree : 	9

Results
 
Correctly Classified Instances         144               96      %
Incorrectly Classified Instances         6                4      %
Kappa statistic                          0.94  
Mean absolute error                      0.035 
Root mean squared error                  0.1586
Relative absolute error                  7.8705 %
Root relative squared error             33.6353 %
Total Number of Instances              150     

=== Confusion Matrix ===

  a  b  c   <-- classified as
 49  1  0 |  a = Iris-setosa
  0 47  3 |  b = Iris-versicolor
  0  2 48 |  c = Iris-virginica

